{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": ""
    }
   },
   "source": [
    "## D7041E - Mini project: Human Real-time Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group: Laura Bermejo, Lina Borg, Julie Labb√©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References and sources\n",
    "For the training of our model, we got inspired by this video: https://www.youtube.com/watch?v=V4Kkrz__hvo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "Some of the required libraries are super-gradient (for model), roboflow (for dataset) and supervision (for bounding boxes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install -q super-gradients <br>\n",
    "pip install -q roboflow <br>\n",
    "pip install -q supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code to download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"DNm47rTel6WBxKZqAd9T\")\n",
    "project = rf.workspace(\"capricon\").project(\"human-detection-q0nit\")\n",
    "dataset = project.version(2).download(\"yolov5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class Config Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "class config:\n",
    "  #Project paths\n",
    "  DATA_DIR: str = \"Human-Detection-2\"\n",
    "  CHECKPOINT_DIR: str = \"checkpoints\"\n",
    "  EXPERIMENT_NAME: str = \"Project\"\n",
    "\n",
    "  #Datasets\n",
    "  TRAIN_IMAGES_DIR: str =\"train/images\"\n",
    "  TRAIN_LABELS_DIR: str = \"train/labels\"\n",
    "  VAL_IMAGES_DIR: str = \"valid/images\"\n",
    "  VAL_LABELS_DIR: str = \"valid/labels\"\n",
    "  TEST_IMAGES_DIR: str = \"test/images\"\n",
    "  TEST_LABELS_DIR: str = \"test/labels\"\n",
    "\n",
    "  #Classes\n",
    "  CLASSES: List[str] = [\"human\", \"vehicle\"]\n",
    "  NUM_CLASSES: int = 2\n",
    "\n",
    "  #Model 1\n",
    "  # DEFINE HYPERPARAMETERS, YOU WILL HAVE TO CHANGE IT\n",
    "  DATALOADER_PARAMS: Dict = {\n",
    "      'batch_size':16,\n",
    "      'num_workers':0\n",
    "  }\n",
    "  # THIS IS ALREADY SET, CHANGE ONLY DATALOADER PARAMS\n",
    "  MODEL_NAME: str = 'yolo_nas_l'\n",
    "  PRETRAINED_WEIGHTS: str ='coco'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloaders initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from super_gradients.training import models, Trainer\n",
    "from super_gradients.training.dataloaders.dataloaders import coco_detection_yolo_format_train, coco_detection_yolo_format_val\n",
    "from super_gradients.training.losses import PPYoloELoss\n",
    "from super_gradients.training.metrics import DetectionMetrics_050\n",
    "from super_gradients.training.models.detection_models.pp_yolo_e import PPYoloEPostPredictionCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = coco_detection_yolo_format_train(\n",
    "    dataset_params={\n",
    "        'data_dir': config.DATA_DIR,\n",
    "        'images_dir': config.TRAIN_IMAGES_DIR,\n",
    "        'labels_dir': config.TRAIN_LABELS_DIR,\n",
    "        'classes': config.CLASSES\n",
    "    },\n",
    "    dataloader_params=config.DATALOADER_PARAMS\n",
    ")\n",
    "\n",
    "test_data = coco_detection_yolo_format_val(\n",
    "    dataset_params={\n",
    "        'data_dir': config.DATA_DIR,\n",
    "        'images_dir': config.TRAIN_IMAGES_DIR,\n",
    "        'labels_dir': config.TRAIN_LABELS_DIR,\n",
    "        'classes': config.CLASSES\n",
    "    },\n",
    "    dataloader_params=config.DATALOADER_PARAMS\n",
    ")\n",
    "\n",
    "val_data = coco_detection_yolo_format_val(\n",
    "    dataset_params={\n",
    "        'data_dir': config.DATA_DIR,\n",
    "        'images_dir': config.TRAIN_IMAGES_DIR,\n",
    "        'labels_dir': config.TRAIN_LABELS_DIR,\n",
    "        'classes': config.CLASSES\n",
    "    },\n",
    "    dataloader_params=config.DATALOADER_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.dataset.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    \"average_best_models\":True,\n",
    "    \"warmup_mode\": \"linear_epoch_step\",\n",
    "    \"warmup_initial_lr\": 1e-6,\n",
    "    \"lr_warmup_epochs\": 3,\n",
    "    \"initial_lr\": 5e-4,\n",
    "    \"lr_mode\": \"cosine\",\n",
    "    \"cosine_final_lr_ratio\": 0.1,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"optimizer_params\": {\"weight_decay\": 0.001},\n",
    "    \"zero_weight_decay_on_bias_and_bn\": True,\n",
    "    \"ema\": True,\n",
    "    \"ema_params\": {\"decay\": 0.9, \"decay_type\": \"threshold\"},\n",
    "    \"max_epochs\": 5,\n",
    "    \"mixed_precision\": True,\n",
    "    \"loss\" : PPYoloELoss (\n",
    "        use_static_assigner=False,\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        reg_max=16\n",
    "    ),\n",
    "\n",
    "    \"valid_metrics_list\": [\n",
    "        DetectionMetrics_050(\n",
    "            score_thres=0.1,\n",
    "            top_k_predictions=300,\n",
    "            num_cls=config.NUM_CLASSES,\n",
    "            normalize_targets=True,\n",
    "            post_prediction_callback=PPYoloEPostPredictionCallback(\n",
    "                score_threshold=0.01,\n",
    "                nms_top_k=1000,\n",
    "                max_predictions=300,\n",
    "                nms_threshold=0.8\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    \"metric_to_watch\": 'mAP@0.50'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.get(config.MODEL_NAME, num_classes=config.NUM_CLASSES, pretrained_weights=config.PRETRAINED_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(experiment_name=config.EXPERIMENT_NAME, ckpt_root_dir=config.CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(model=model, training_params=train_params, train_loader=train_data, valid_loader=val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Charge best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "avg_model = models.get(config.MODEL_NAME, num_classes=config.NUM_CLASSES, checkpoint_path=os.path.join(config.CHECKPOINT_DIR, config.EXPERIMENT_NAME, 'average_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with test_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(\n",
    "    model=avg_model, \n",
    "    test_loader=test_data, \n",
    "    test_metrics_list=DetectionMetrics_050(\n",
    "        score_thres=0.1, \n",
    "        top_k_predictions=300, \n",
    "        num_cls=config.NUM_CLASSES, \n",
    "        normalize_targets=True, \n",
    "        post_prediction_callback=PPYoloEPostPredictionCallback(\n",
    "            score_threshold=0.01,\n",
    "            nms_top_k=1000,\n",
    "            max_predictions=300,\n",
    "            nms_threshold=0.7\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import supervision as sv\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sv.Dataset.from_yolo(images_directory_path=f\"{config.DATA_DIR}/test/images\", annotations_directory_path=f\"{config.DATA_DIR}/test/labels\", data_yaml_path=f\"{config.DATA_DIR}/data.yaml\", force_masks=False)\n",
    "predictions = {}\n",
    "CONFIDENCE_THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_name,image in ds.images.items():\n",
    "    result = list(avg_model.predict(image, conf=CONFIDENCE_THRESHOLD))[0]\n",
    "    detections = sv.Detections(xyxy=result.prediction.bboxes_xyxy, confidence=result.prediction.confidence, class_id=result.prediction.labels.astype(int))\n",
    "    predictions[image_name] = detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot annotations vs predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_IMAGE_COUNT = 10\n",
    "n = min(MAX_IMAGE_COUNT, len(ds.images))\n",
    "\n",
    "keys = list(ds.images.keys())\n",
    "keys = random.sample(keys,n)\n",
    "\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "\n",
    "images = []\n",
    "titles = []\n",
    "\n",
    "for key in keys:\n",
    "    frame_with_annotations = box_annotator.annotate(scene=ds.images[key].copy (), detections =  ds. annotations [key], skip_label=True)\n",
    "    images.append(frame_with_annotations)\n",
    "    titles.append('annotations')\n",
    "    frame_with_predictions = box_annotator. annotate( scene = ds.images[key].copy (),detections = predictions [key], skip_label=True)\n",
    "    images.append(frame_with_predictions)\n",
    "    titles.append('predictions')\n",
    "\n",
    "%matplotlib inline\n",
    "sv.plot_images_grid(images-images, titles-titles, grid_size=(n, 2), size=(2*4, n*4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Systematically chosen hyper-parameters\n",
    "We are using the random search to systematically chose the hyper-parameters. Since it takes a long time to train the model, we are only searching for the best combination of two parameters, but we can definitely use the same process for more of them. Actually, the random search would even work better with more parameters and more iterations, but as mentioned, it takes a while to do.\n",
    "\n",
    "We decided to test multiple initial learning rates which has an impact on how much the parameters will update. We also test multiple weight decay for the optimizer. The optimizer `Adam` uses weight decay to penalize large weights to reduce overfitting tendencies. A higher value of weight decay will penalize more the large weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Hyperparameter search space\n",
    "hyperparam_space = {\n",
    "    \"initial_lr\": [3e-4, 5e-4, 7e-4],\n",
    "    \"optimizer_params\": [{\"weight_decay\": 0.0015}, {\"weight_decay\": 0.001}, {\"weight_decay\": 0.002}]\n",
    "}\n",
    "\n",
    "# Random search\n",
    "num_iterations = 5\n",
    "best_score = -float('inf')\n",
    "best_hyperparams = None\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Randomly sample hyperparameters from the search space\n",
    "    train_params[\"initial_lr\"] = np.random.choice(hyperparam_space[\"initial_lr\"])\n",
    "    train_params[\"optimizer_params\"] = np.random.choice(hyperparam_space[\"optimizer_params\"])\n",
    "    # Train the model using the sampled hyperparameters and get the score\n",
    "    trainer.train(model=model, training_params=train_params, train_loader=train_data, valid_loader=val_data)\n",
    "    test_results = trainer.test(model=model, test_loader=test_data, test_metrics_list=[\n",
    "        #Accuracy(),\n",
    "        DetectionMetrics_050(\n",
    "            score_thres=0.1, \n",
    "            top_k_predictions=300, \n",
    "            num_cls=config.NUM_CLASSES, \n",
    "            normalize_targets=True, \n",
    "            post_prediction_callback=PPYoloEPostPredictionCallback(\n",
    "                score_threshold=0.01, \n",
    "                nms_top_k=1000, \n",
    "                max_predictions=300,\n",
    "                nms_threshold=0.7\n",
    "            )\n",
    "        )\n",
    "    ])\n",
    "    print(f\"Test results: DetectionMetrics: {test_results['DetectionMetrics_050']}\")\n",
    "    #score = test_results['Accuracy']\n",
    "\n",
    "    # Check if the current score is better than the previous best\n",
    "    #if score > best_score:\n",
    "        #best_score = score\n",
    "        #best_hyperparams = {\"initial_lr\": train_params[\"initial_lr\"], \"optimizer_params\": train_params[\"optimizer_params\"]}\n",
    "#print(best_score)\n",
    "#print(best_hyperparams)\n",
    "\n",
    "# Use the best parameters for the next step\n",
    "#train_params[\"initial_lr\"] = best_hyperparams[\"initial_lr\"]\n",
    "#train_params[\"optimizer_params\"] = best_hyperparams[\"optimizer_params\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation\n",
    "When we first downloaded the data, it got separated into three folders: test, train, validate. For cross-validation, we will use `KFold` method from `scikit-learn` which divides our data into folds. More specifically, it will divide the training data into multiples folds for training and one for test data. However, since we already have a testing folder, we will disregard the one generated by the function `KFold``. We will also disregard the validate folder, as the process of cross-validation already does validation with one of the folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance of different seeds\n",
    "The function `KFold` accepts a parameter `random_state` to set a seed for the randomization of the elements in each fold. More specifically, the items in the dataset are shuffled before getting split. We will test three different seeds and evaluate their performance through precision and recall metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def read_data(path):\n",
    "    dataset = []\n",
    "    for filename in os.listdir(path):\n",
    "        file_path = os.path.join(path, filename)\n",
    "        if \"images\" in path:\n",
    "            file = cv2.imread(file_path)\n",
    "        if \"labels\" in path:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    lines = file.readlines()\n",
    "                    file = []\n",
    "                    for line in lines:\n",
    "                        values = line.strip().split()\n",
    "                        try:\n",
    "                            # Convert label to integer (values[0]) and ensure rest are floats\n",
    "                            label = int(float(values[0]))\n",
    "                            bbox_coords = [float(val) for val in values[1:]]\n",
    "                            annotation = [label] + bbox_coords\n",
    "                            file.append(annotation)\n",
    "                        except ValueError as e:\n",
    "                            print(f\"Error parsing line '{line}' of file {file_path}: {e}\")\n",
    "                    file = np.array(file)\n",
    "        dataset.append(file)\n",
    "    return np.array(dataset)\n",
    "\n",
    "def write_data(dataset, path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    for i, file in enumerate(dataset):\n",
    "        if \"images\" in path:\n",
    "            filename = f\"{i}.jpg\"\n",
    "            output_path = os.path.join(path, filename)\n",
    "            cv2.imwrite(output_path, file)\n",
    "        if \"labels\" in path:\n",
    "            for i, annotation in enumerate(dataset):\n",
    "                filename = f\"{i}.txt\"\n",
    "                output_path = os.path.join(path, filename)\n",
    "\n",
    "                with open(output_path, 'w') as file:\n",
    "                    for value in annotation:\n",
    "                        # Convert label to integer and keep bounding box coordinates as floats\n",
    "                        label = str(int(value[0]))\n",
    "                        bbox_coords = ' '.join([str(coord) for coord in value[1:]])\n",
    "                        line = label + ' ' + bbox_coords\n",
    "                        file.write(line)\n",
    "                        file.write('\\n')\n",
    "\n",
    "def cross_validation(values, labels, seed):\n",
    "    current_iteration = 0\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    num_folds = 3\n",
    "    kf = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "\n",
    "    for train_index, test_index in kf.split(values):\n",
    "        values_train, _ = values[train_index], values[test_index]\n",
    "        labels_train, _ = labels[train_index], labels[test_index]\n",
    "\n",
    "        # In order to use the training function, we need to store the images and labels in respective folders\n",
    "        output_path = dataset.location + f\"/train_fold_{current_iteration}\"\n",
    "        write_data(values_train, output_path + \"/images\")\n",
    "        write_data(labels_train, output_path + \"/labels\")\n",
    "        \n",
    "        # Converts our new separated datasets into Dataloaders\n",
    "        train_data = coco_detection_yolo_format_train(\n",
    "            dataset_params={\n",
    "                'data_dir': config.DATA_DIR,\n",
    "                'images_dir': f\"train_fold_{current_iteration}/images\", \n",
    "                'labels_dir': f\"train_fold_{current_iteration}/labels\",\n",
    "                'classes': config.CLASSES,\n",
    "                'show_all_warnings': True,\n",
    "            },\n",
    "            dataloader_params=config.DATALOADER_PARAMS\n",
    "        )\n",
    "        # Train and test\n",
    "        trainer.train(model=model, training_params=train_params, train_loader=train_data, valid_loader=val_data)\n",
    "        test_results = trainer.test(model=model, test_loader=test_data, test_metrics_list=[\n",
    "            #Average(),\n",
    "            DetectionMetrics_050(\n",
    "                score_thres=0.1, \n",
    "                top_k_predictions=300, \n",
    "                num_cls=config.NUM_CLASSES, \n",
    "                normalize_targets=True, \n",
    "                post_prediction_callback=PPYoloEPostPredictionCallback(\n",
    "                    score_threshold=0.01, \n",
    "                    nms_top_k=1000, \n",
    "                    max_predictions=300,\n",
    "                    nms_threshold=0.7\n",
    "                )\n",
    "            )\n",
    "        ])\n",
    "        print(f\"Test results: DetectionMetrics: {test_results['DetectionMetrics_050']}\")\n",
    "        #accuracies.append(test_results['DetectionMetrics_050'])\n",
    "\n",
    "        current_iteration += 1\n",
    "\n",
    "    #average_accuracy = sum(accuracies) / len(accuracies)\n",
    "    #print(f\"Average accuracy for seed {seed} = {average_accuracy}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data in order to split it in folds\n",
    "path = dataset.location + \"/train\"\n",
    "loaded_dataset = read_data(path + \"/images\")\n",
    "loaded_labels = read_data(path + \"/labels\")\n",
    "seeds = [1, 2, 3]\n",
    "for seed in seeds:\n",
    "    cross_validation(loaded_dataset, loaded_labels, seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
